{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Loading data\n",
    "\n",
    "The following code shows how to load the datasets for this project.  \n",
    "Among which, we do not release the labels (the \"stars\" column) for the test set. \n",
    "You may evaluate your trained model on the validation set instead.\n",
    "However, your submitted predictions (``pred.csv``) should be generated on the test set.\n",
    "\n",
    "Each year we release different data, so old models are not guaranteed to solve the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split_name='train', columns=['text', 'stars'], folder='data'):\n",
    "    '''\n",
    "        \"split_name\" may be set as 'train', 'valid' or 'test' to load the corresponding dataset.\n",
    "        \n",
    "        You may also specify the column names to load any columns in the .csv data file.\n",
    "        Among many, \"text\" can be used as model input, and \"stars\" column is the labels (sentiment). \n",
    "        If you like, you are free to use columns other than \"text\" for prediction.\n",
    "    '''\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"Success\")\n",
    "        return df\n",
    "    except:\n",
    "        print(f\"Failed loading specified columns... Returning all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can extract the data by specifying the desired split and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the train split\n",
      "Success\n",
      "select [text, stars] columns from the valid split\n",
      "Success\n",
      "select [text, stars] columns from the test split\n",
      "Failed loading specified columns... Returning all columns from the test split\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text', 'stars'], folder='data')\n",
    "valid_df = load_data('valid', columns=['text', 'stars'], folder='data')\n",
    "# the test set labels (the 'stars' column) are not available! So the following code will instead return all columns\n",
    "test_df = load_data('test', columns=['text', 'stars'], folder='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RobertA\n",
    "\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.model_max_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize dataset\n",
    "def tokenize(text):\n",
    "  return tokenizer(text, truncation=True,max_length=512,padding='max_length')\n",
    "\n",
    "def tokenize_df(df):\n",
    "  tokens = df['text'].map(tokenize)\n",
    "  df['input_ids'] = [x['input_ids'] for x in tokens]\n",
    "  df['attention_mask'] = [x['attention_mask'] for x in tokens]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tokenize_df(train_df)\n",
    "valid_df = tokenize_df(valid_df)\n",
    "test_df = tokenize_df(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 5e-5\n",
    "dropout_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "  def __init__(self,checkpoint,num_labels): \n",
    "    super(CustomModel,self).__init__() \n",
    "    self.num_labels = num_labels \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "    self.dropout = nn.Dropout(dropout_prob) \n",
    "    self.classifier = nn.Linear(768,num_labels) # load and initialize weights\n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "    #Extract outputs from the body\n",
    "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    #Add custom layers\n",
    "    sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "\n",
    "    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
    "    \n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      loss_fct = nn.CrossEntropyLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    \n",
    "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-emotion were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=CustomModel(checkpoint=checkpoint,num_labels=5).to(device)\n",
    "# model=CustomModel(checkpoint=\"checkpoints/model_bs16_lr5e-05_dop0.5.pth\",num_labels=5).to(device)\n",
    "model.load_state_dict(torch.load(\"checkpoints/model_bs16_lr5e-05_dop0.5.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        assert len(df['input_ids']) == len(df['stars'])\n",
    "        self.input_ids = df['input_ids']\n",
    "        self.attention_mask = df['attention_mask']\n",
    "        self.label = df['stars']-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return np.asarray(self.input_ids[idx]), np.asarray(self.attention_mask[idx]), self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(MyDataset(train_df), batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(MyDataset(valid_df), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jkimbf/miniconda3/envs/comp4471/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW,get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1125/1125 [05:27<00:00,  3.43it/s, train_loss=0.0511, train_acc=0.66] \n",
      "100%|██████████| 125/125 [00:11<00:00, 10.85it/s, val_loss=0.0423, val_acc=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84       282\n",
      "           1       0.52      0.26      0.35       136\n",
      "           2       0.56      0.46      0.50       212\n",
      "           3       0.55      0.56      0.56       466\n",
      "           4       0.81      0.87      0.84       904\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.65      0.61      0.62      2000\n",
      "weighted avg       0.70      0.71      0.70      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[256  15   9   0   2]\n",
      " [ 52  36  41   6   1]\n",
      " [  9  16  97  86   4]\n",
      " [  6   1  26 259 174]\n",
      " [  4   1   1 116 782]]\n",
      "epoch  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1125/1125 [05:37<00:00,  3.33it/s, train_loss=0.0393, train_acc=0.735]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.90it/s, val_loss=0.0449, val_acc=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.83       282\n",
      "           1       0.37      0.50      0.42       136\n",
      "           2       0.61      0.29      0.39       212\n",
      "           3       0.61      0.37      0.46       466\n",
      "           4       0.76      0.93      0.84       904\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.62      0.60      0.59      2000\n",
      "weighted avg       0.69      0.70      0.67      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[254  21   3   1   3]\n",
      " [ 56  68   7   4   1]\n",
      " [  6  83  62  48  13]\n",
      " [  5  12  29 174 246]\n",
      " [  6   1   1  58 838]]\n",
      "epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1125/1125 [05:43<00:00,  3.28it/s, train_loss=0.0302, train_acc=0.809]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.79it/s, val_loss=0.0461, val_acc=0.719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85       282\n",
      "           1       0.47      0.38      0.42       136\n",
      "           2       0.59      0.49      0.54       212\n",
      "           3       0.59      0.50      0.54       466\n",
      "           4       0.80      0.87      0.83       904\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.65      0.63      0.64      2000\n",
      "weighted avg       0.70      0.72      0.71      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[264  12   3   0   3]\n",
      " [ 58  51  23   4   0]\n",
      " [  7  44 104  49   8]\n",
      " [  4   2  41 232 187]\n",
      " [  6   0   4 108 786]]\n",
      "epoch  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1125/1125 [05:43<00:00,  3.28it/s, train_loss=0.0197, train_acc=0.884]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.87it/s, val_loss=0.0544, val_acc=0.723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83       282\n",
      "           1       0.46      0.53      0.49       136\n",
      "           2       0.61      0.54      0.57       212\n",
      "           3       0.59      0.53      0.56       466\n",
      "           4       0.81      0.86      0.84       904\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.66      0.66      0.66      2000\n",
      "weighted avg       0.72      0.72      0.72      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[233  42   4   2   1]\n",
      " [ 36  72  24   3   1]\n",
      " [  5  35 115  51   6]\n",
      " [  2   3  42 246 173]\n",
      " [  3   3   5 113 780]]\n",
      "epoch  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1125/1125 [05:35<00:00,  3.35it/s, train_loss=0.0117, train_acc=0.939]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.90it/s, val_loss=0.0637, val_acc=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       282\n",
      "           1       0.44      0.48      0.46       136\n",
      "           2       0.59      0.56      0.58       212\n",
      "           3       0.58      0.53      0.56       466\n",
      "           4       0.81      0.86      0.83       904\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.65      0.65      0.65      2000\n",
      "weighted avg       0.71      0.72      0.72      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[230  45   5   0   2]\n",
      " [ 36  65  31   3   1]\n",
      " [  3  32 119  53   5]\n",
      " [  3   2  42 248 171]\n",
      " [  2   3   4 121 774]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):    \n",
    "    print('epoch ', epoch+1)\n",
    "    model.train()\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    train_count = 0\n",
    "    with tqdm.tqdm(train_dataloader) as t:\n",
    "        for input_ids, attention_mask, labels in t:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids,attention_mask,labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            train_acc += (torch.argmax(outputs.logits,dim=-1) == labels).sum().item()\n",
    "            train_count += labels.size(0)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            t.set_postfix({'train_loss': train_loss/train_count, 'train_acc': train_acc/train_count})\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    val_acc = 0\n",
    "    val_count = 0\n",
    "    val_loss = 0\n",
    "    with tqdm.tqdm(valid_dataloader) as t:\n",
    "        for input_ids, attention_mask, labels in t:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids,attention_mask,labels)\n",
    "                loss = outputs.loss\n",
    "            val_acc += (torch.argmax(outputs.logits,dim=-1) == labels).sum().item()\n",
    "            val_count += len(labels)\n",
    "            val_loss += loss.item()\n",
    "            y_pred += torch.argmax(outputs.logits,dim=-1).tolist()\n",
    "            y_true += labels.tolist()\n",
    "\n",
    "            t.set_postfix({'val_loss': val_loss/val_count, 'val_acc': val_acc/val_count})\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    torch.save(model.state_dict(), 'checkpoints/model_2l_bs{}_lr{}_dop{}_e{}.pth'.format(batch_size, learning_rate, dropout_prob, epoch+1))\n",
    "    with open('logs/model_2l_bs{}_lr{}_dop{}_e{}.txt'.format(batch_size, learning_rate, dropout_prob, epoch), 'w') as f:\n",
    "        f.write(str(classification_report(y_true, y_pred)))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(str(confusion_matrix(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoints/model_bs{}_lr{}_dop{}.pth'.format(batch_size, learning_rate, dropout_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('model1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:12<00:00,  9.92it/s, val_loss=0.0475, val_acc=0.728]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "val_acc = 0\n",
    "val_count = 0\n",
    "val_loss = 0\n",
    "with tqdm.tqdm(valid_dataloader) as t:\n",
    "    for input_ids, attention_mask, labels in t:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids,attention_mask,labels)\n",
    "            loss = outputs.loss\n",
    "        val_acc += (torch.argmax(outputs.logits,dim=-1) == labels).sum().item()\n",
    "        val_count += len(labels)\n",
    "        val_loss += loss.item()\n",
    "        y_pred += torch.argmax(outputs.logits,dim=-1).tolist()\n",
    "        y_true += labels.tolist()\n",
    "\n",
    "        t.set_postfix({'val_loss': val_loss/val_count, 'val_acc': val_acc/val_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85       282\n",
      "           1       0.50      0.51      0.51       136\n",
      "           2       0.58      0.56      0.57       212\n",
      "           3       0.59      0.56      0.57       466\n",
      "           4       0.82      0.86      0.84       904\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.67      0.66      0.67      2000\n",
      "weighted avg       0.72      0.73      0.73      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[235  37   7   1   2]\n",
      " [ 28  70  35   3   0]\n",
      " [  2  29 118  59   4]\n",
      " [  2   2  39 259 164]\n",
      " [  5   3   5 117 774]]\n"
     ]
    }
   ],
   "source": [
    "with open('logs/model_bs{}_lr{}_dop{}.txt'.format(batch_size, learning_rate, dropout_prob), 'w') as f:\n",
    "    # f.write(str(classification_report(y_true, y_pred)))\n",
    "    # f.write(\"\\n\\n\")\n",
    "    # f.write(str(confusion_matrix(y_true, y_pred)))\n",
    "    print(str(classification_report(y_true, y_pred)))\n",
    "    print(\"\\n\\n\")\n",
    "    print(str(confusion_matrix(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Saving predictions to file\n",
    "\n",
    "Your submitted predictions are supposed to be a .csv file containing two columns, i.e. (``review_id`` and ``stars``). \n",
    "\n",
    "Here, as an example, we generate some random predictions as our answer, which are put in a DataFrame and output to a .csv file\n",
    "\n",
    "After getting your model predictions on the test set, you may follow these steps to generate your ``pred.csv`` file. (By replacing the random predictions with your model predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        assert len(df['input_ids']) == len(df['attention_mask'])\n",
    "        self.input_ids = df['input_ids']\n",
    "        self.attention_mask = df['attention_mask']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return np.asarray(self.input_ids[idx]), np.asarray(self.attention_mask[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "test_dataloader = DataLoader(MyTestDataset(test_df), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:22<00:00, 11.09it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = []\n",
    "with tqdm.tqdm(test_dataloader) as t:\n",
    "    for input_ids, attention_mask in t:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids,attention_mask)\n",
    "        y_pred += torch.argmax(outputs.logits,dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['stars'] = [x+1 for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IKcZpSuELli7DUjU2fKGNg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-04-07 17:17:39</td>\n",
       "      <td>0</td>\n",
       "      <td>I77zZlSdCFAClxdjHwPcxw</td>\n",
       "      <td>OMG! I'm an avid spray tanner and have been al...</td>\n",
       "      <td>2</td>\n",
       "      <td>tUZtqzqE0bIOcLelcR4opg</td>\n",
       "      <td>[0, 3765, 534, 328, 38, 437, 41, 20137, 11782,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vbVJzKDhHlhMnKRpES5QzQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-30 17:42:40</td>\n",
       "      <td>0</td>\n",
       "      <td>ioFNKarf29KGjRZdH0qC8Q</td>\n",
       "      <td>Sets the standard. Authentic. Outstanding. Cou...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gwvrebru-kDM1N51aeJiFg</td>\n",
       "      <td>[0, 104, 2580, 5, 2526, 4, 41808, 636, 4, 2548...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GdPWJo3z4ySEXpF7Wkn3FA</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-02 05:53:47</td>\n",
       "      <td>2</td>\n",
       "      <td>9429anmcYIcaEcMptJCNKQ</td>\n",
       "      <td>Came on 7/23/2014 with a group of 10 - service...</td>\n",
       "      <td>1</td>\n",
       "      <td>at7dS8gtLiEwd_4uHv231A</td>\n",
       "      <td>[0, 347, 4344, 15, 262, 73, 1922, 73, 16310, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BYc5IFQPq-PLVXnYjDp6vw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-20 19:31:21</td>\n",
       "      <td>0</td>\n",
       "      <td>PsUCdt7PKjzgBC0c7xXhJA</td>\n",
       "      <td>I love Bobs Subs! Tasty n made to order...yum!...</td>\n",
       "      <td>0</td>\n",
       "      <td>vaQxpV8IXqRmCIAHovP4NA</td>\n",
       "      <td>[0, 100, 657, 3045, 29, 4052, 29, 328, 255, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wxxvi3LZbHNIDwJ-ZimtnA</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-06-27 00:44:08</td>\n",
       "      <td>0</td>\n",
       "      <td>GQBlykKyShQcNeu2ivLdSA</td>\n",
       "      <td>This is my hotel of choice on the strip.  I re...</td>\n",
       "      <td>0</td>\n",
       "      <td>dy_4NAZ0KR2bDoB9qAOMRg</td>\n",
       "      <td>[0, 713, 16, 127, 2303, 9, 2031, 15, 5, 9572, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  IKcZpSuELli7DUjU2fKGNg     1  2015-04-07 17:17:39      0   \n",
       "1  vbVJzKDhHlhMnKRpES5QzQ     1  2017-06-30 17:42:40      0   \n",
       "2  GdPWJo3z4ySEXpF7Wkn3FA     0  2014-08-02 05:53:47      2   \n",
       "3  BYc5IFQPq-PLVXnYjDp6vw     0  2015-02-20 19:31:21      0   \n",
       "4  Wxxvi3LZbHNIDwJ-ZimtnA     0  2012-06-27 00:44:08      0   \n",
       "\n",
       "                review_id                                               text  \\\n",
       "0  I77zZlSdCFAClxdjHwPcxw  OMG! I'm an avid spray tanner and have been al...   \n",
       "1  ioFNKarf29KGjRZdH0qC8Q  Sets the standard. Authentic. Outstanding. Cou...   \n",
       "2  9429anmcYIcaEcMptJCNKQ  Came on 7/23/2014 with a group of 10 - service...   \n",
       "3  PsUCdt7PKjzgBC0c7xXhJA  I love Bobs Subs! Tasty n made to order...yum!...   \n",
       "4  GQBlykKyShQcNeu2ivLdSA  This is my hotel of choice on the strip.  I re...   \n",
       "\n",
       "   useful                 user_id  \\\n",
       "0       2  tUZtqzqE0bIOcLelcR4opg   \n",
       "1       1  Gwvrebru-kDM1N51aeJiFg   \n",
       "2       1  at7dS8gtLiEwd_4uHv231A   \n",
       "3       0  vaQxpV8IXqRmCIAHovP4NA   \n",
       "4       0  dy_4NAZ0KR2bDoB9qAOMRg   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [0, 3765, 534, 328, 38, 437, 41, 20137, 11782,...   \n",
       "1  [0, 104, 2580, 5, 2526, 4, 41808, 636, 4, 2548...   \n",
       "2  [0, 347, 4344, 15, 262, 73, 1922, 73, 16310, 1...   \n",
       "3  [0, 100, 657, 3045, 29, 4052, 29, 328, 255, 19...   \n",
       "4  [0, 713, 16, 127, 2303, 9, 2031, 15, 5, 9572, ...   \n",
       "\n",
       "                                      attention_mask  stars  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      5  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      5  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      5  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      4  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(f'test_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(data={\n",
    "    'review_id': test_df['review_id'],\n",
    "    'stars': test_df['stars']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I77zZlSdCFAClxdjHwPcxw</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ioFNKarf29KGjRZdH0qC8Q</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9429anmcYIcaEcMptJCNKQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PsUCdt7PKjzgBC0c7xXhJA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GQBlykKyShQcNeu2ivLdSA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars\n",
       "0  I77zZlSdCFAClxdjHwPcxw      5\n",
       "1  ioFNKarf29KGjRZdH0qC8Q      5\n",
       "2  9429anmcYIcaEcMptJCNKQ      1\n",
       "3  PsUCdt7PKjzgBC0c7xXhJA      5\n",
       "4  GQBlykKyShQcNeu2ivLdSA      4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(f'pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
