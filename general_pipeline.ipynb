{"cells":[{"cell_type":"markdown","metadata":{"id":"rplMkz_8XbF8"},"source":["# General pipeline for project 1\n","This is an example pipeline showing you how to  \n","(1) Load the provided data;  \n","(2) Train models on the train set, and use the validation set to evaluate your model performance;  \n","(3) Generate predictions (pred.csv) on the test set, which is ready for submission."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"TBytrFziXbF_","executionInfo":{"status":"ok","timestamp":1648048866233,"user_tz":-480,"elapsed":1180,"user":{"displayName":"Sun Bin Mun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjni6Be3E6-BcUDwLqOX8Om0qT90_3szZCq9Gx6JA=s64","userId":"11452487959529512770"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_JYid1z7Xdcn","executionInfo":{"status":"ok","timestamp":1648048903845,"user_tz":-480,"elapsed":6078,"user":{"displayName":"Sun Bin Mun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjni6Be3E6-BcUDwLqOX8Om0qT90_3szZCq9Gx6JA=s64","userId":"11452487959529512770"}},"outputId":"f6d3a7d0-ca94-4d0b-cc6a-0b9f1903a823"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"qORlp3jMXbGA"},"source":["### (1) Loading data\n","The following code shows how to load the datasets for this project.  \n","Among which, we do not release the labels (the \"stars\" column) for the test set. You may evaluate your trained model on the validation set instead.\n","\n","However, your submitted predictions (``pred.csv``) should be generated on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1FmG0chXbGB"},"outputs":[],"source":["def load_data(split_name='train', columns=['text', 'stars'], folder='data'):\n","    '''\n","        \"split_name\" may be set as 'train', 'valid' or 'test' to load the corresponding dataset.\n","        \n","        You may also specify the column names to load any columns in the .csv data file.\n","        Among many, \"text\" can be used as model input, and \"stars\" column is the labels (sentiment). \n","        If you like, you are free to use columns other than \"text\" for prediction.\n","    '''\n","    try:\n","        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n","        df = pd.read_csv(f'{folder}/{split_name}.csv')\n","        df = df.loc[:,columns]\n","        print(\"Success\")\n","        return df\n","    except:\n","        print(f\"Failed loading specified columns... Returning all columns from the {split_name} split\")\n","        df = pd.read_csv(f'{folder}/{split_name}.csv')\n","        return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gg8_oftzXbGC","outputId":"4e837b0d-d0ca-46d2-ab75-2aad3fc11ff9"},"outputs":[{"name":"stdout","output_type":"stream","text":["select [text, stars] columns from the train split\n","Success\n","select [text, stars] columns from the valid split\n","Success\n","select [text, stars] columns from the test split\n","Failed loading specified columns... Returning all columns from the test split\n"]}],"source":["train_df = load_data('train', columns=['text', 'stars'])\n","valid_df = load_data('valid', columns=['text', 'stars'])\n","# the test set labels (the 'stars' column) are not available! So the following code will instead return all columns\n","test_df = load_data('test', columns=['text', 'stars'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZ4eqNoxXbGD","outputId":"3ce1dc27-7741-4ee7-82bd-b0fbb6b80be9"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>business_id</th>\n","      <th>cool</th>\n","      <th>date</th>\n","      <th>funny</th>\n","      <th>review_id</th>\n","      <th>text</th>\n","      <th>useful</th>\n","      <th>user_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>IKcZpSuELli7DUjU2fKGNg</td>\n","      <td>1</td>\n","      <td>2015-04-07 17:17:39</td>\n","      <td>0</td>\n","      <td>I77zZlSdCFAClxdjHwPcxw</td>\n","      <td>OMG! I'm an avid spray tanner and have been al...</td>\n","      <td>2</td>\n","      <td>tUZtqzqE0bIOcLelcR4opg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>vbVJzKDhHlhMnKRpES5QzQ</td>\n","      <td>1</td>\n","      <td>2017-06-30 17:42:40</td>\n","      <td>0</td>\n","      <td>ioFNKarf29KGjRZdH0qC8Q</td>\n","      <td>Sets the standard. Authentic. Outstanding. Cou...</td>\n","      <td>1</td>\n","      <td>Gwvrebru-kDM1N51aeJiFg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GdPWJo3z4ySEXpF7Wkn3FA</td>\n","      <td>0</td>\n","      <td>2014-08-02 05:53:47</td>\n","      <td>2</td>\n","      <td>9429anmcYIcaEcMptJCNKQ</td>\n","      <td>Came on 7/23/2014 with a group of 10 - service...</td>\n","      <td>1</td>\n","      <td>at7dS8gtLiEwd_4uHv231A</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BYc5IFQPq-PLVXnYjDp6vw</td>\n","      <td>0</td>\n","      <td>2015-02-20 19:31:21</td>\n","      <td>0</td>\n","      <td>PsUCdt7PKjzgBC0c7xXhJA</td>\n","      <td>I love Bobs Subs! Tasty n made to order...yum!...</td>\n","      <td>0</td>\n","      <td>vaQxpV8IXqRmCIAHovP4NA</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wxxvi3LZbHNIDwJ-ZimtnA</td>\n","      <td>0</td>\n","      <td>2012-06-27 00:44:08</td>\n","      <td>0</td>\n","      <td>GQBlykKyShQcNeu2ivLdSA</td>\n","      <td>This is my hotel of choice on the strip.  I re...</td>\n","      <td>0</td>\n","      <td>dy_4NAZ0KR2bDoB9qAOMRg</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3995</th>\n","      <td>nqKLJ9q7jvnzRKW4RIu3qw</td>\n","      <td>0</td>\n","      <td>2015-08-28</td>\n","      <td>0</td>\n","      <td>LJ7Fqb2hlrGhYL-HK_VGAQ</td>\n","      <td>I have a glass wall and ceiling sunroom by Fou...</td>\n","      <td>0</td>\n","      <td>nuxXEP96hVyEjmZv2QZ1fQ</td>\n","    </tr>\n","    <tr>\n","      <th>3996</th>\n","      <td>Avr_qHllB60Ade8IwnU7Nw</td>\n","      <td>0</td>\n","      <td>2013-10-21 00:59:29</td>\n","      <td>0</td>\n","      <td>M1SmtK0Fy2GPA26yzrfGWg</td>\n","      <td>I love Kohls. My favorite place to clothes sho...</td>\n","      <td>0</td>\n","      <td>nURWL_BFa3Mp90ZtmmGakQ</td>\n","    </tr>\n","    <tr>\n","      <th>3997</th>\n","      <td>PsYU8cEDE3oewzKCyVt5bw</td>\n","      <td>0</td>\n","      <td>2016-07-05 02:10:44</td>\n","      <td>0</td>\n","      <td>wSI7X7qqMfDQxkNFHkOYwA</td>\n","      <td>Wow, this was genuinely terrible food. I order...</td>\n","      <td>1</td>\n","      <td>9gX_LpIjPD3McOdKz8ixNQ</td>\n","    </tr>\n","    <tr>\n","      <th>3998</th>\n","      <td>4_oBvzWuJe3_Y8PRUaPmug</td>\n","      <td>0</td>\n","      <td>2015-09-26 18:32:56</td>\n","      <td>0</td>\n","      <td>DQjRGs939gBPqfoIqK7VYQ</td>\n","      <td>Worst service ever! My clothes were tagged wro...</td>\n","      <td>1</td>\n","      <td>mPFcpFlSG4A964z7aPJchg</td>\n","    </tr>\n","    <tr>\n","      <th>3999</th>\n","      <td>HTr5ZMFUihWQ8Sai0ikg4A</td>\n","      <td>1</td>\n","      <td>2013-06-03 04:46:05</td>\n","      <td>2</td>\n","      <td>LleiJNhBSvjfwloSKd7xvA</td>\n","      <td>Love/love/love Tina for pink/white.  I recentl...</td>\n","      <td>1</td>\n","      <td>FISzlNZgrFylvgX3Nl_a5w</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4000 rows Ã— 8 columns</p>\n","</div>"],"text/plain":["                 business_id  cool                 date  funny  \\\n","0     IKcZpSuELli7DUjU2fKGNg     1  2015-04-07 17:17:39      0   \n","1     vbVJzKDhHlhMnKRpES5QzQ     1  2017-06-30 17:42:40      0   \n","2     GdPWJo3z4ySEXpF7Wkn3FA     0  2014-08-02 05:53:47      2   \n","3     BYc5IFQPq-PLVXnYjDp6vw     0  2015-02-20 19:31:21      0   \n","4     Wxxvi3LZbHNIDwJ-ZimtnA     0  2012-06-27 00:44:08      0   \n","...                      ...   ...                  ...    ...   \n","3995  nqKLJ9q7jvnzRKW4RIu3qw     0           2015-08-28      0   \n","3996  Avr_qHllB60Ade8IwnU7Nw     0  2013-10-21 00:59:29      0   \n","3997  PsYU8cEDE3oewzKCyVt5bw     0  2016-07-05 02:10:44      0   \n","3998  4_oBvzWuJe3_Y8PRUaPmug     0  2015-09-26 18:32:56      0   \n","3999  HTr5ZMFUihWQ8Sai0ikg4A     1  2013-06-03 04:46:05      2   \n","\n","                   review_id  \\\n","0     I77zZlSdCFAClxdjHwPcxw   \n","1     ioFNKarf29KGjRZdH0qC8Q   \n","2     9429anmcYIcaEcMptJCNKQ   \n","3     PsUCdt7PKjzgBC0c7xXhJA   \n","4     GQBlykKyShQcNeu2ivLdSA   \n","...                      ...   \n","3995  LJ7Fqb2hlrGhYL-HK_VGAQ   \n","3996  M1SmtK0Fy2GPA26yzrfGWg   \n","3997  wSI7X7qqMfDQxkNFHkOYwA   \n","3998  DQjRGs939gBPqfoIqK7VYQ   \n","3999  LleiJNhBSvjfwloSKd7xvA   \n","\n","                                                   text  useful  \\\n","0     OMG! I'm an avid spray tanner and have been al...       2   \n","1     Sets the standard. Authentic. Outstanding. Cou...       1   \n","2     Came on 7/23/2014 with a group of 10 - service...       1   \n","3     I love Bobs Subs! Tasty n made to order...yum!...       0   \n","4     This is my hotel of choice on the strip.  I re...       0   \n","...                                                 ...     ...   \n","3995  I have a glass wall and ceiling sunroom by Fou...       0   \n","3996  I love Kohls. My favorite place to clothes sho...       0   \n","3997  Wow, this was genuinely terrible food. I order...       1   \n","3998  Worst service ever! My clothes were tagged wro...       1   \n","3999  Love/love/love Tina for pink/white.  I recentl...       1   \n","\n","                     user_id  \n","0     tUZtqzqE0bIOcLelcR4opg  \n","1     Gwvrebru-kDM1N51aeJiFg  \n","2     at7dS8gtLiEwd_4uHv231A  \n","3     vaQxpV8IXqRmCIAHovP4NA  \n","4     dy_4NAZ0KR2bDoB9qAOMRg  \n","...                      ...  \n","3995  nuxXEP96hVyEjmZv2QZ1fQ  \n","3996  nURWL_BFa3Mp90ZtmmGakQ  \n","3997  9gX_LpIjPD3McOdKz8ixNQ  \n","3998  mPFcpFlSG4A964z7aPJchg  \n","3999  FISzlNZgrFylvgX3Nl_a5w  \n","\n","[4000 rows x 8 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# test_df.columns\n","# print(train_df.columns)\n","# print(valid_df.columns)\n","# print(test_df.columns)\n","test_df"]},{"cell_type":"markdown","metadata":{"id":"qXHHL86wXbGD"},"source":["### (2) Training and validating \n","The following example shows you how to train your model using the train set, and evaluate on the validation set.  \n","As an example, we only use the text data for training. Feel free to use other columns in your implementation.  \n","\n","The model performance on the validation set can be roughly regarded as your models final performance, so we can use it to search for optimal hyper-parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qr3GJsvsXbGE"},"outputs":[],"source":["# Prepare the data.\n","# As an example, we only use the text data. \n","x_train = train_df['text']\n","y_train = train_df['stars']\n","  \n","x_valid = valid_df['text']\n","y_valid = valid_df['stars']\n","\n","x_test = test_df['text']"]},{"cell_type":"markdown","metadata":{"id":"jO9b1638XbGE"},"source":[" You can use the valid data to choose the hyperparameters.\n","As an example, you can decide which value of C (1 or 100) is better by evaluating on the valid data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmRS9GkGXbGF"},"outputs":[],"source":["# build the first linear model with TFIDF feature\n","tfidf = TfidfVectorizer()\n","lr1 = LogisticRegression(C=100)\n","steps = [('tfidf', tfidf),('lr', lr1)]\n","pipe1 = Pipeline(steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDVuFLqMXbGF","outputId":"d7b96b33-6ef2-4df6-c4c1-eb9b44a01153"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/jchengaj/miniconda3/envs/comp4332/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"data":{"text/plain":["Pipeline(steps=[('tfidf', TfidfVectorizer()),\n","                ('lr', LogisticRegression(C=100))])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# train the first model\n","pipe1.fit(x_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X34d2mGRXbGG","outputId":"e80faabd-06ca-4e3d-d7bd-0fc60c76863d"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           1       0.70      0.72      0.71       282\n","           2       0.30      0.27      0.29       136\n","           3       0.37      0.40      0.38       212\n","           4       0.47      0.46      0.47       466\n","           5       0.77      0.77      0.77       904\n","\n","    accuracy                           0.62      2000\n","   macro avg       0.52      0.52      0.52      2000\n","weighted avg       0.62      0.62      0.62      2000\n","\n","\n","\n","\n","[[202  35  23   7  15]\n"," [ 45  37  32  15   7]\n"," [ 19  32  85  56  20]\n"," [  8  14  62 216 166]\n"," [ 14   4  30 164 692]]\n","accuracy 0.616\n"]}],"source":["# validate on the validation set\n","y_pred = pipe1.predict(x_valid)\n","print(classification_report(y_valid, y_pred))\n","print(\"\\n\\n\")\n","print(confusion_matrix(y_valid, y_pred))\n","print('accuracy', np.mean(y_valid == y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmmBciQ6XbGG"},"outputs":[],"source":["# build the second linear model with TFIDF feature\n","tfidf = TfidfVectorizer()\n","lr2 = LogisticRegression(C=1)\n","steps = [('tfidf', tfidf),('lr', lr2)]\n","pipe2 = Pipeline(steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMiJcucFXbGG","outputId":"528af049-314c-4fda-e627-8090793904c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/jchengaj/miniconda3/envs/comp4332/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"data":{"text/plain":["Pipeline(steps=[('tfidf', TfidfVectorizer()), ('lr', LogisticRegression(C=1))])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# train the second model\n","pipe2.fit(x_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"hl2XzBSKXbGH","outputId":"67c9bbd6-a53f-493f-e11f-82bef95d9390"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           1       0.70      0.81      0.75       282\n","           2       0.35      0.18      0.23       136\n","           3       0.48      0.32      0.38       212\n","           4       0.51      0.46      0.48       466\n","           5       0.74      0.86      0.79       904\n","\n","    accuracy                           0.65      2000\n","   macro avg       0.55      0.52      0.53      2000\n","weighted avg       0.63      0.65      0.63      2000\n","\n","\n","\n","\n","[[228  13   5  12  24]\n"," [ 50  24  36  13  13]\n"," [ 21  21  67  74  29]\n"," [ 18   6  24 214 204]\n"," [ 11   5   8 107 773]]\n","accuracy 0.653\n"]}],"source":["# validate on the validation set\n","y_pred = pipe2.predict(x_valid)\n","print(classification_report(y_valid, y_pred))\n","print(\"\\n\\n\")\n","print(confusion_matrix(y_valid, y_pred))\n","print('accuracy', np.mean(y_valid == y_pred))"]},{"cell_type":"markdown","metadata":{"id":"jY43ZHk6XbGH"},"source":[" We find the second model (pipe2) has higher accuracy, then we use the second model to make predictions on test data. In practice, you may not only focus on the accuracy, but also other metrics (precision, recall, f1), since the label distribution is not always balanced."]},{"cell_type":"markdown","metadata":{"id":"FMktjOClXbGI"},"source":["### (3) Generate predictions on the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKmsRNxvXbGI"},"outputs":[],"source":["predict_test = pipe2.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDo96c4_XbGI","outputId":"7dddb2e8-16d9-4bc0-96cf-8fd0e927916d"},"outputs":[{"data":{"text/plain":["array([5, 5, 1, ..., 1, 1, 2])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["predict_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRdEEgl_XbGI"},"outputs":[],"source":["# save your model predictions\n","pred_df = pd.DataFrame({'stars': predict_test, 'review_id': test_df['review_id']})\n","pred_df.to_csv('pred.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"uTifkfhuXbGJ"},"source":[" Then you may submit the predictions `pred.csv` on the test set. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"name":"general_pipeline.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}